{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Homework 3\n",
    "\n",
    "### 1.\n",
    "\n",
    "We consider the LASSO problem :\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\min_w & \\frac{1}{2}\\vert\\vert Xw - y \\vert\\vert^2_2 + \\lambda \\vert\\vert w \\vert\\vert_1\\\\\n",
    "\\iff \\min_{w, z} & \\frac{1}{2} \\vert\\vert z \\vert\\vert^2_2 +  \\lambda \\vert\\vert w \\vert\\vert_1\\\\\n",
    "     \\text{s.t.} & z = Xw - y\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "where we introduce the dummy variable $z$ to compute the dual of the problem in a useful form.\n",
    "\n",
    "Indeed, using this form of the problem, the Lagrangian is, with $\\nu$ denoting the Lagrange multiplier :\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\mathscr{L} (w, z, \\nu) &= \\frac{1}{2} \\vert\\vert z \\vert\\vert^2_2 + \\lambda \\vert\\vert w \\vert\\vert_1 + \\nu^T\\left( z - Xw + y \\right)\\\\\n",
    "&= \\frac{1}{2} \\vert\\vert z \\vert\\vert^2_2 + \\nu^T z + \\lambda \\left( \\vert\\vert w \\vert\\vert_1 - \\frac{1}{\\lambda} \\left( X^T\\nu \\right)^T w \\right) + \\nu^T y\\\\\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "Minimizing $\\mathscr{L}(z, w, \\nu)$ over $w$ is equivalent to :\n",
    "\n",
    "$$\\max_w \\lambda \\left( \\frac{1}{\\lambda} \\left( X^T\\nu \\right)^T w - \\vert\\vert w \\vert\\vert_1 \\right)$$\n",
    "\n",
    "where we recognize the conjugate norm $\\lambda \\vert\\vert \\frac{1}{\\lambda} X^T \\nu \\vert\\vert_1^*$, therefore :\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\min_{w} \\mathscr{L} (w, z, \\nu) &= -\\infty \\text{ if } \\vert\\vert \\frac{1}{\\lambda} X^T \\nu \\vert\\vert_{\\infty} > 1\\\\\n",
    "                             &= \\frac{1}{2} \\vert\\vert z \\vert\\vert^2_2 + \\nu^T z + \\nu^T y \\text{ otherwise}\\\\\n",
    "                             &= z^T \\left( \\frac{1}{2}z + \\nu \\right) + \\nu^T y\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "We compute the gradient of $\\mathscr{L}$ with respect to $z$ to minimize it (i.e. compute the Lagrange dual function $g(\\nu)$) :\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\nabla \\mathscr{L}(z) &= \\frac{1}{2} z + \\nu + \\frac{1}{2}z\\\\\n",
    "                      &= z + \\nu\\\\\n",
    "\\implies \\nabla \\mathscr{L}(z) &= 0 \\iff z = -\\nu\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "Therefore, we obtain :\n",
    "\n",
    "$$g(\\nu) = -\\frac{1}{2} \\nu^T\\nu + \\nu^Ty$$\n",
    "\n",
    "and the dual problem expressed as a quadratic program is :\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\min_{v} \\, -g(v) &= v^T Q v + p^T v\\\\\n",
    "       \\text{s.t. } Av &\\leq b\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "where $v = \\nu, Q = \\frac{1}{2} I_n, p = -y,$\n",
    "\\begin{align}\n",
    "A &= \\begin{bmatrix}\n",
    "X^T\\\\\n",
    "-X^T\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "and $b = \\lambda \\mathbb{1}_{2d}$ (which is $\\vert\\vert \\frac{1}{\\lambda} X^T\\nu \\vert\\vert_\\infty \\leq 1$ written in matrix form)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.\n",
    "\n",
    "Let us now implement the barrier method to solve this dual problem.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Union, Tuple\n",
    "import sys\n",
    "import cvxpy as cvx  # to check if we get the correct solutions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Backtracking Line Search params\n",
    "BLS_ALPHA = 0.1\n",
    "BLS_BETA = 0.7\n",
    "\n",
    "# Newton precision\n",
    "NEWTON_EPSILON = 1e-8\n",
    "\n",
    "LOGBARR_MU = 10.  # path update : t <- Âµ * t\n",
    "LOGBARR_T0 = 1.  # first t value (start of path)\n",
    "\n",
    "# Fix seed for reproducible tests\n",
    "# np.random.seed(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def log_barrier(vals: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Log barrier function. Expects NEGATIVE values as inputs.\n",
    "    (This is the function \"phi\" from slide 7 of Lecture 5)\n",
    "\n",
    "    :param vals: value of constraint functions\n",
    "    :return: log barrier evaluation\n",
    "    \"\"\"\n",
    "    if np.any(vals >= 0):\n",
    "        raise ValueError('[log_barrier] Positive value encountered')\n",
    "\n",
    "    return -np.sum(np.log(-vals))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def check_objective_def(A: np.ndarray,\n",
    "                        b: np.ndarray,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a function that checks if the objective is defined\n",
    "\n",
    "    :param A: the inequality constraint matrix\n",
    "    :param b: the inequality constraint vector\n",
    "    :return: A function of x that returns True if Ax - b < 0\n",
    "    \"\"\"\n",
    "    def _check(x):\n",
    "        u = A @ x\n",
    "        u = u - b\n",
    "        return np.all(u < 0)\n",
    "\n",
    "    return _check"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bls(f: Callable,\n",
    "        gradf_x: Union[np.ndarray, float],\n",
    "        x: Union[np.ndarray, float],\n",
    "        delta_x: Union[np.ndarray, float],\n",
    "        alpha: float,\n",
    "        beta: float,\n",
    "        check_def: Callable,\n",
    "        ) -> float:\n",
    "    \"\"\"\n",
    "    Backtracking Line Search algorithm.\n",
    "    The algorithm may not converge if we are very close to the optimum,\n",
    "    therefore we break out of the loop if the step length becomes zero\n",
    "    (the loop becomes infinite in this case)\n",
    "\n",
    "    :param f: the objective function (real-valued)\n",
    "    :param gradf_x: the gradient of f at x\n",
    "    :param x: the current point from which to descent\n",
    "    :param delta_x: the descent direction\n",
    "    :param alpha: parameter in [0, 1/2]\n",
    "    :param beta: shrinkage parameter in [0, 1)\n",
    "    :param check_def: function that returns True on an input v if f(v) is defined\n",
    "    :return: the optimal (if reached) step length t\n",
    "    \"\"\"\n",
    "    t = 1.\n",
    "\n",
    "    while (\n",
    "            not check_def(x + t * delta_x)\n",
    "            or f(x + t * delta_x) >= f(x) + alpha * t * np.dot(gradf_x, delta_x)\n",
    "    ):\n",
    "        t = beta * t\n",
    "        if t < sys.float_info.epsilon:\n",
    "            break\n",
    "\n",
    "    return t"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Computation of the gradient and the Hessian\n",
    "\n",
    "\n",
    "The gradient and Hessian of $g_t(v) = t\\left( v^TQv + p^Tv \\right) - \\sum\\limits_{k=1}^{2d} \\log \\left( b - Av \\right)_k$ are :\n",
    "\n",
    "$$\\nabla g_t(v) = t(2Qv + p) - \\sum_{k=1}^{2d} \\frac{1}{(Av - b)_k} A^T_k$$\n",
    "\n",
    "where $A^T_k$ is the $k$-th column of $A^T$ i.e. the $k$-th line of $A$, and\n",
    "\n",
    "$$\\nabla^2 g_t(v) = t2Q + \\sum_{k=1}^{2d} \\frac{1}{(Av - b)_k^2} M(k)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$M(k)_{ij} = A_{ki} A_{kj}$$\n",
    "\n",
    "i.e. $M(k)$ is the \"tensor product\" of the $k$-th line of $A$ with itself :\n",
    "\n",
    "\\begin{align}\n",
    "M(k) &= \\begin{bmatrix}\n",
    "A_{k1}\\\\\n",
    "\\vdots\\\\\n",
    "A_{kn}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "A_{k1} \\hdots A_{kn}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Using the element-wise multiplication rule of NumPy arrays, these weighted sums are written in short expressions in the code below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def centering_step(\n",
    "        Q: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        A: np.ndarray,\n",
    "        b: np.ndarray,\n",
    "        t: float,\n",
    "        v0: np.ndarray,\n",
    "        eps: float\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform the centering step of the log-barrier algorithm.\n",
    "    Compute the steepest descent in local Hessian norm (Newton algorithm)\n",
    "    and choose a step length using backtracking line search\n",
    "\n",
    "    :param Q: Hessian of the original objective\n",
    "    :param p: linear coefficients of the original objective\n",
    "    :param A: Constraint matrix\n",
    "    :param b: Constraint vector\n",
    "    :param t: Current log-barrier parameter\n",
    "    :param v0: Starting point\n",
    "    :param eps: Desired precision\n",
    "    :return: the history of points of the Newton descent\n",
    "    \"\"\"\n",
    "\n",
    "    objective_f = lambda v: t * (v.T @ Q @ v + p @ v) + log_barrier(A @ v - b)\n",
    "    def_checker = check_objective_def(A, b)\n",
    "    v = v0\n",
    "    v_history = [v]\n",
    "    previous_decrement = None  # debug purposes (check decreasing decrement)\n",
    "\n",
    "    while True:\n",
    "        # Compute gradient and Hessian of the objective\n",
    "        u = A @ v - b\n",
    "        grad = t * (2 * Q @ v + p) - np.sum(1 / u * A.T, axis=1)  # sum( 1/(Av-b)[k]*(k-th line of A) )\n",
    "\n",
    "        barrier_hess = (1 / u**2 * A.T) @ A\n",
    "        hess = t * 2 * Q + barrier_hess\n",
    "\n",
    "        delta_v = np.linalg.solve(hess, -grad)  # Newton descent direction\n",
    "        newton_decrement_sq = np.dot(grad, -delta_v)\n",
    "\n",
    "        # Sanity check\n",
    "        if previous_decrement is None:\n",
    "            previous_decrement = newton_decrement_sq\n",
    "        if newton_decrement_sq - previous_decrement > 0.:\n",
    "            print('[WARNING][centering_step] Newton decrement increasing')\n",
    "\n",
    "        # Stop\n",
    "        if 0.5 * newton_decrement_sq <= eps:\n",
    "            return np.array(v_history, dtype=float)\n",
    "\n",
    "        # Descend\n",
    "        step_l = bls(\n",
    "            f=objective_f,\n",
    "            gradf_x=grad,\n",
    "            x=v,\n",
    "            delta_x=delta_v,\n",
    "            alpha=BLS_ALPHA,\n",
    "            beta=BLS_BETA,\n",
    "            check_def=def_checker,\n",
    "        )\n",
    "        reached_zero = step_l < sys.float_info.epsilon\n",
    "        if reached_zero:\n",
    "            return np.array(v_history, dtype=float)\n",
    "\n",
    "        v = v + step_l * delta_v\n",
    "        v_history.append(v)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def barr_method(\n",
    "        Q: np.ndarray,\n",
    "        p: np.ndarray,\n",
    "        A: np.ndarray,\n",
    "        b: np.ndarray,\n",
    "        v0: np.ndarray,\n",
    "        eps: float,\n",
    "        mu=LOGBARR_MU,\n",
    "        t0=LOGBARR_T0\n",
    ") -> np.ndarray:\n",
    "    def_checker = check_objective_def(A, b)\n",
    "    if not def_checker(v0):\n",
    "        raise ValueError('The log barrier is not defined for the input v0')\n",
    "\n",
    "    m = len(b)\n",
    "    t = t0\n",
    "    v_history = [v0]\n",
    "    v = v0\n",
    "\n",
    "    while True:\n",
    "        v = centering_step(Q=Q,\n",
    "                           p=p,\n",
    "                           A=A,\n",
    "                           b=b,\n",
    "                           t=t,\n",
    "                           v0=v,\n",
    "                           eps=NEWTON_EPSILON)[-1]\n",
    "\n",
    "        v_history.append(v)\n",
    "\n",
    "        if m / t < eps:\n",
    "            break\n",
    "\n",
    "        t = mu * t\n",
    "\n",
    "    return np.array(v_history, dtype=float)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.\n",
    "\n",
    "We now test the implemented LASSO solver using random data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_random_data(n_samples, dim, scale, w_density, y_dev) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate random data with sparse linear relationship.\n",
    "\n",
    "    :param n_samples: # of samples drawn\n",
    "    :param dim: dimension of each sample\n",
    "    :param scale: bounds samples' coordinates between [-scale, +scale]\n",
    "    :param w_density: density of the relationship (ie density of w_star)\n",
    "    :param y_dev: deviation of observations with the linear relationship\n",
    "    :return: X : matrix of samples\n",
    "             y : observations (labels)\n",
    "             w_star : sparse vector encoding the linear relationship\n",
    "    \"\"\"\n",
    "    X = scale * 2 * (np.random.rand(n_samples, dim) - 0.5)\n",
    "    w_star = np.random.randn(dim)\n",
    "    idx_nonzero = np.random.choice(range(dim), int((1 - w_density) * dim), replace=False)\n",
    "    for i in idx_nonzero:\n",
    "        w_star[i] = 0.\n",
    "    y = X @ w_star + np.random.normal(0, y_dev, size=n_samples)\n",
    "\n",
    "    return X, y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A very small example\n",
    "n = 3\n",
    "d = 2\n",
    "scale = 10.\n",
    "X, y = generate_random_data(n, d, scale=scale, w_density=0.4, y_dev=0.5)\n",
    "_lambda = 10.\n",
    "\n",
    "Q = 0.5 * np.eye(n)\n",
    "p = -y\n",
    "A = np.vstack([X.T, -X.T])\n",
    "b = np.full(shape=2 * d, fill_value=_lambda)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "v0 = np.zeros(shape=n)  # v0 stricly feasible (for all lambda > 0)\n",
    "v_hist = barr_method(Q=Q,\n",
    "                     p=p,\n",
    "                     A=A,\n",
    "                     b=b,\n",
    "                     v0=v0,\n",
    "                     eps=1e-16)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dual_objective(v):\n",
    "    res = np.zeros(len(v), dtype=float)\n",
    "    for i, _v in enumerate(v):\n",
    "        res[i] = np.dot(_v, np.dot(Q, _v)) + np.dot(p, _v)\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.semilogy(dual_objective(v_hist[:-1]) - dual_objective([v_hist[-1]]), label=f'Dual objective $f$ ($\\mu = {LOGBARR_MU})$')\n",
    "plt.legend()\n",
    "plt.ylabel('$f(v_t) - f^*$')\n",
    "_ = plt.xlabel('# Log-barrier iterations')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We propose to solve the dual with cvxpy to ensure we get the same result\n",
    "# Create dual\n",
    "v = cvx.Variable(shape=n, name='v')\n",
    "_Q = cvx.Parameter(shape=Q.shape, name='Q', PSD=True)\n",
    "_Q.value = Q\n",
    "_p = cvx.Parameter(shape=p.shape, name='p')\n",
    "_p.value = p\n",
    "dual_constraints = [A @ v <= b]\n",
    "\n",
    "dual_obj_cvx = cvx.Minimize(cvx.quad_form(x=v, P=_Q) + _p.T @ v)\n",
    "dual_problem = cvx.Problem(dual_obj_cvx, dual_constraints)\n",
    "dual_problem.solve()\n",
    "\n",
    "print(f'CVXPy Status : {dual_problem.status}\\n'\n",
    "      f'Optimal value as found by CVXPy :              {dual_problem.value}\\n'\n",
    "      f'Optimal value as found by the barrier method : {dual_objective([v_hist[-1]])[0]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected, a linear convergence in semilog scale, and we see that we converged to the same value as CVXPy : this log-barrier implementation thus works as intended. Moreover, we see that the precision threshold of the log-barrier behaves correctly by looking at the decimals of the optima."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A more complex example\n",
    "n = 50\n",
    "d = 300\n",
    "\n",
    "scale = 1.\n",
    "X, y = generate_random_data(n, d, scale, w_density=0.4, y_dev=0.5)\n",
    "_lambda = 10.\n",
    "\n",
    "Q = 0.5 * np.eye(n)\n",
    "p = -y\n",
    "A = np.vstack([X.T, -X.T])\n",
    "b = np.full(shape=2 * d, fill_value=_lambda)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'X^T X is singular : {np.linalg.cond(X.T @ X) >= 1 / sys.float_info.epsilon}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "v0 = np.zeros(shape=n)  # v0 stricly feasible (for all lambda > 0)\n",
    "v_hist = barr_method(Q=Q,\n",
    "                     p=p,\n",
    "                     A=A,\n",
    "                     b=b,\n",
    "                     v0=v0,\n",
    "                     eps=1e-16)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.semilogy(dual_objective(v_hist[:-1]) - dual_objective([v_hist[-1]]), label=f'Dual objective $f$ ($\\mu = {LOGBARR_MU})$')\n",
    "plt.legend()\n",
    "plt.ylabel('$f(v_t) - f^*$')\n",
    "_ = plt.xlabel('# Log-barrier iterations')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dual\n",
    "v = cvx.Variable(shape=n, name='v')\n",
    "_Q = cvx.Parameter(shape=Q.shape, name='Q', PSD=True)\n",
    "_Q.value = Q\n",
    "_p = cvx.Parameter(shape=p.shape, name='p')\n",
    "_p.value = p\n",
    "dual_constraints = [A @ v <= b]\n",
    "\n",
    "dual_obj_cvx = cvx.Minimize(cvx.quad_form(x=v, P=_Q) + _p.T @ v)\n",
    "dual_problem = cvx.Problem(dual_obj_cvx, dual_constraints)\n",
    "dual_problem.solve()\n",
    "\n",
    "# N.B. : the displayed values are optimal values for the LASSO dual,\n",
    "# which are the opposite of (QP)'s optimal value.\n",
    "print(f'CVXPy Status : {dual_problem.status}\\n'\n",
    "      f'Optimal value as found by CVXPy :              {-dual_problem.value}\\n'\n",
    "      f'Optimal value as found by the barrier method : {-dual_objective([v_hist[-1]])[0]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Still a very quick convergence, both in number of iterations and in time !"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We must now relate the optimum of the dual with the optimum of the primal. The primal\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\min_{w, z} & \\frac{1}{2} \\vert\\vert z \\vert\\vert^2_2 +  \\lambda \\vert\\vert w \\vert\\vert_1\\\\\n",
    "     \\text{s.t. } & z = Xw - y\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "is convex (by convexity of norms in the objective function, and convexity of the linear inequality constraints), and Slater's condition is verified since the domain of the objective and the (non-existent) inequality constraint is $(w, z) \\in \\mathcal{D} = \\mathbb{R}^d \\times \\mathbb{R}^n$, which is open. Therefore, any feasible point is in the interior of $\\mathcal{D}$. Moreover, the computation of the Lagrange dual function established relationships between the primal and dual variables :\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "z^* &= -\\nu^* \\text{ by definition of the Lagrange dual function (from } \\nabla \\mathscr{L}(z, \\nu ) = 0 \\iff z = -\\nu \\text{ )}\\\\\n",
    "X w^* &= z^* + y \\text{ by constraints}\\\\\n",
    "\\iff w^* &= \\left( X^T X \\right)^{-1} X^T (z^* + y) \\text{ if rank}(X) = d\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "In practice, if $n < d$, we always have $\\text{rank}(X) < d$. However, solving the system to get a value of $w^*$ still results in a solution of the primal with the same residues. As highlighted by [Tibshirani (2012), The Lasso Problem and Uniqueness](https://arxiv.org/abs/1206.0313), it may be difficult to choose the \"right\" $w^*$ value as it is difficult to define such a notion, particularly when different candidates have different non-zero coefficients (supports). In the same paper, Tibshirani derives a sufficient condition for uniqueness from the KKT conditions (pages 4, 5) : considering the set\n",
    "\n",
    "$$\\varepsilon = \\{ i \\in \\{1, ..., d\\}, \\vert X_i^T (X w^* - y) \\vert = \\lambda \\}$$\n",
    "\n",
    "where $X_i \\in \\mathbb{R}^n$ denotes a column of $X$, and the matrix $X_\\varepsilon$ defined by the concatenation of the columns $X_j, j \\in \\varepsilon$, then the solution is unique if $\\text{rank}(X_\\varepsilon) = \\text{Card} (\\varepsilon)$, and, denoting $s$ the vector containing the signs of the elements of $X_\\varepsilon^T (y - Xw^*)$ :\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "w^*_\\varepsilon &= \\left( X_\\varepsilon^T X_\\varepsilon \\right)^{-1} \\left( X^T_\\varepsilon y - \\lambda s \\right)\\\\\n",
    "w^*_{\\bar{\\varepsilon}} &= 0\\\\\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "where $w^*_\\varepsilon$ denotes the vector of components of $w^*$ whose indices are in $\\varepsilon$\n",
    "\n",
    "We can try to apply this criterion to get the solution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_epsilon_set(X,\n",
    "                        resid,\n",
    "                        _lambda,\n",
    "                        tol=1e-12\n",
    "                        ):\n",
    "    X_cols = X.T\n",
    "    epsilon_set = []\n",
    "    for i, col in enumerate(X_cols):\n",
    "        if abs(abs((np.dot(col, resid))) - _lambda) < tol:\n",
    "            epsilon_set.append(i)\n",
    "\n",
    "    return epsilon_set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eps_set = compute_epsilon_set(X, -v_hist[-1], _lambda=_lambda)\n",
    "print(f'Size of epsilon set : {len(eps_set)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_w_epsilon(X,\n",
    "                      resid,\n",
    "                      y,\n",
    "                      _lambda,\n",
    "                      ):\n",
    "    w_epsilon = np.zeros(shape=len(X[0]), dtype=float)\n",
    "    eps_set = compute_epsilon_set(X, resid, _lambda)\n",
    "    if not eps_set:\n",
    "        print(f'[WARNING] epsilon set is empty. Returning an array of zeroes')\n",
    "        return w_epsilon\n",
    "\n",
    "    X_epsilon = X.T[eps_set].T\n",
    "    s = np.sign(np.dot(X_epsilon.T, -resid)) # -resid is y - Xw\n",
    "\n",
    "    w_epsilon_nonzero = np.dot(np.linalg.inv(X_epsilon.T @ X_epsilon), np.dot(X_epsilon.T, y) - _lambda * s)\n",
    "    for i, loc in enumerate(eps_set):\n",
    "        w_epsilon[loc] = w_epsilon_nonzero[i]\n",
    "\n",
    "    return w_epsilon"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w_eps = compute_w_epsilon(X, resid=-v_hist[-1], y=y, _lambda=_lambda)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def primal_objective(X, w, y, _lambda):\n",
    "    return 0.5 * np.dot(X @ w - y, X @ w - y)  + _lambda * np.linalg.norm(w, ord=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compare with cvxpy\n",
    "# Create primal\n",
    "w = cvx.Variable(shape=d, name='w')\n",
    "w.value = np.zeros(shape=d, dtype=float)\n",
    "\n",
    "lambd = cvx.Parameter(nonneg=True, name='lambda')\n",
    "lambd.value = _lambda\n",
    "\n",
    "primal_obj_cvx = cvx.Minimize(\n",
    "    cvx.Constant(0.5) * cvx.norm2(X @ w - y)**2 + lambd * cvx.norm1(w)\n",
    ")\n",
    "\n",
    "primal_problem = cvx.Problem(primal_obj_cvx)\n",
    "primal_problem.solve()\n",
    "print(f'CVXPy Status : {primal_problem.status}\\n'\n",
    "      f'Primal optimal value as found by CVXPy : {primal_problem.value}\\n'\n",
    "      f'Primal objective at w_epsilon :          {primal_objective(X, w_eps, y, _lambda)}\\n'\n",
    "      f'Dual optimum as found by CVXPy :         {-dual_problem.value}\\n'\n",
    "      f'Dual optimum as found by log barrier :   {-dual_objective([v_hist[-1]])[0]}\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When $\\lambda$ is big, the epsilon set is empty, since the residues aren't large enough with respect to $\\lambda$. For lower values of $\\lambda$, we get $\\varepsilon \\neq \\emptyset$. Note that $\\varepsilon = \\emptyset$ does not imply that the solution is not unique, as the condition $\\text{rank}\\left(X_\\varepsilon\\right) = \\text{Card}\\,\\varepsilon$ is only a sufficient condition.\n",
    "\n",
    "In fact, even in cases where $\\varepsilon = \\emptyset$, we still get the same optimum as CVXPy, indicating that $w = 0$ is in fact the optimal solution when its $l1$-norm is heavily penalized w.r.t residues.\n",
    "\n",
    "Below is a comparison of solutions $w^*$ obtained with a linear system solution by NumPy (somehow working ? we still have $X^T X$ singular...)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z_star = -v_hist[-1]\n",
    "w_star = np.linalg.solve(X.T @ X, np.dot(X.T, z_star + y))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compare w_star and w_eps\n",
    "print(f'l1-norm of w_epsilon :  {np.linalg.norm(w_eps, ord=1)}\\n'\n",
    "      f'l1-norm of w_star :     {np.linalg.norm(w_star, ord=1)}\\n'\n",
    "      f'Squared norm of residues associated to w_eps :  {np.linalg.norm(np.dot(X, w_eps) - y)**2}\\n'\n",
    "      f'Squared norm of residues associated to w_star : {np.linalg.norm(np.dot(X, w_star) - y)**2}\\n'\n",
    "      f'Primal objective at w_epsilon : {primal_objective(X, w_eps, y, _lambda)}\\n'\n",
    "      f'Primal objective at w_star    : {primal_objective(X, w_star, y, _lambda)}\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For large values of $\\lambda$, we clearly see that the solution provided by NumPy is very far from the optimal, as it has a very large $l1$-norm in comparison to that of $w_\\varepsilon$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us now study the impact of $\\mu$ on the optimal $w^* \\triangleq w_\\varepsilon$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mus = np.array([1.1, 2., 10., 30., 70., 100., 130., 350.])\n",
    "histories = []\n",
    "z_stars = []\n",
    "w_stars = []\n",
    "\n",
    "for mu in mus:\n",
    "    histories.append(barr_method(\n",
    "        Q,\n",
    "        p,\n",
    "        A,\n",
    "        b,\n",
    "        v0,\n",
    "        eps=1e-16,\n",
    "        mu=mu,\n",
    "        t0=LOGBARR_T0,\n",
    "    ))\n",
    "\n",
    "    z_stars.append(-histories[-1][-1])\n",
    "\n",
    "    w_stars.append(compute_w_epsilon(X, z_stars[-1], y, _lambda))\n",
    "\n",
    "z_stars = np.array(z_stars)\n",
    "w_stars = np.array(w_stars)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'All w_stars are close to each other : {np.allclose(w_stars, [w_stars[0] for _ in range(len(w_stars))], atol=1e-12)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$w^*$ doesn't seem to be impacted at all by the choice of $\\mu$, therefore, we can simply choose any value of $\\mu$ for which the convergence is empirically fast.\n",
    "Moreover, the above check seems to show that the solution to the problem is in fact unique, as predicted by Lemma 4 in Tibshirani 2012, which states that if the points are drawn from a continuous distribution on $\\mathbb{R}^{np}$, the solution is almost surely unique. Since we draw points uniformly, we indeed get a unique optimum."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Optimal value of primal at (any) w_star : {primal_objective(X, w_stars[0], y, _lambda)}\\n'\n",
    "      f'L1-norm of w_star                       : {np.linalg.norm(w_stars[0])}\\n'\n",
    "      f'Sparsity (number of non-zeros)          : {np.count_nonzero(w_stars[0] != 0)} out of {len(w_stars[0])} components')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
